{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import CSVLogger, TBXLogger\n",
    "from rllib_utils.env import launch_and_wrap_env\n",
    "from rllib_utils.rllib_callbacks import MyCallbacks\n",
    "from rllib_utils.rllib_loggers import TensorboardImageLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configs\n",
    "\n",
    "We configure 3 different tools:\n",
    "- 1. Duckietown environment\n",
    "- 2. Ray Tune\n",
    "- 3. Ray PPO hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Envirnoment config\n",
    "\n",
    "The Gym-Duckietown environment provides a wide palette of possible configurations.\n",
    "Most of the chosen config settings is detailed in the below setup.\n",
    "\n",
    "For more details visit: https://github.com/duckietown/gym-duckietown\n",
    "\n",
    "\n",
    "Note, that this time the ```domain_adaptation``` settings is True. This triggers the DomainAdaptationWrapper from ```observation_wrappers.py``` to wrap the Duckietown Gym environmant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = {\n",
    "    # Run mode allows to load different settings in different run modes. Available options: 'train', 'inference', 'debug'\n",
    "    \"mode\": 'train',\n",
    "    # Length of an episode (if not terminated due to a failure)\n",
    "    \"episode_max_steps\": 500,\n",
    "    # The input image will be scaled to (height, width)\n",
    "    \"resized_input_shape\" : '(64, 64)',\n",
    "    # Crop the top part of the image\n",
    "    \"crop_image_top\": False,  # The top 1/top_crop_divider part of the image will be cropped off. (E.g. 3 crops the top third of the image)\n",
    "    \"top_crop_divider\": 3,\n",
    "    # Convert the image to grayscale\n",
    "    \"grayscale_image\": False,\n",
    "    # Stack multiple frames as input\n",
    "    \"frame_stacking\": True,\n",
    "    # Number of frames to stack if frame stacking is enabled\n",
    "    \"frame_stacking_depth\": 3,\n",
    "    # Apply motion blur to the images during training\n",
    "    \"motion_blur\": False,\n",
    "    # Map the action space to a certain type. Available options are:\n",
    "    # 'leftright', leftright_clipped, 'leftright_braking', steering_braking, 'discrete'\n",
    "    # 'heading', 'heading_smooth', 'heading_trapz', 'heading_sine', 'heading_limited',\n",
    "    \"action_type\": 'heading',\n",
    "    # Overwrite the default reward function of Gym Duckietown ('default' leaves the default reward of the gym)\n",
    "    # Available options: 'default', 'default_clipped', 'posangle', 'lane_distance'\n",
    "    \"reward_function\": 'posangle',\n",
    "    # Set Gym Duckietown's distortion parameter to generate fisheye distorted images\n",
    "    \"distortion\": True,\n",
    "    # How large ange deviation should be accepted when the robot is placed into the simulator\n",
    "    \"accepted_start_angle_deg\": 4,\n",
    "    \"simulation_framerate\": 20,\n",
    "    # Skip frames in the agent-environment loop and only step the environment using the last action\n",
    "    \"frame_skip\": 1,\n",
    "    # Computed actions come into effect this much later in the time period of a step.\n",
    "    # Allowed values: floats in the (0., 1.) interval or 'random' to get random values in each instance of the env\n",
    "    \"action_delay_ratio\": 0.0,\n",
    "    # Map(s) used during training. Individual map names could be specified or 'multimap1' to train on a custom map set\n",
    "    \"training_map\": 'multi',\n",
    "    # Use Gym Duckietown's domain randomization\n",
    "    \"domain_rand\": False,\n",
    "    \"dynamics_rand\": False,\n",
    "    \"camera_rand\": False,\n",
    "    # If >0.0 a new observation/frame will be the same as the previous one, with a probability of frame_repeating\n",
    "    \"frame_repeating\" : 0.0,\n",
    "    # Spawn obstacles (duckies, duckiebots, etc.) to random drivable positions on a map (with or without fixed obstacles)\n",
    "    # To spawn other duckiebots this option is not recommended, use spawn_forward_obstacle instead\n",
    "    # Type and amount of spawned obstacles is determined by the dictionary under the 'obstacles' key\n",
    "    \"spawn_obstacles\": False,\n",
    "    \"obstacles\" :{\n",
    "      # Keys at this level specify the type of object to be spawned.\n",
    "      # Supported options: 'duckie', 'duckiebot', 'cone', 'barrier'\n",
    "      \"duckie\" :{\n",
    "          # Density: Amount of ducks per drivable tile. Can't be larger than one currently\n",
    "          \"density\": 0.5,\n",
    "          # Non-static objects move according to their default behaviour implemented in the gym Duckietown\n",
    "          # Duckies \"walk\" back and forth (like crossing the road), duckiebots perform lane following.\n",
    "          \"static\": True},\n",
    "      \"duckiebot\" :{\n",
    "          \"density\": 0,\n",
    "          \"static\": False}},\n",
    "    # Spawn a duckiebot in front of the controlled robot in every episode\n",
    "    # Parameters of the robot are randomised, but settings are hardcoded in ForwardObstacleSpawnnigWrapper)\n",
    "    \"spawn_forward_obstacle\": False,\n",
    "    # Evaluators of the AI driving olympics use small steps to generate many frames which are overlayed to get blured images\n",
    "    # The aido wrapper implements this blur, and also implements the same dynamics simulation\n",
    "    #Warning: Using AIDOWrapper slows down the environment simulation (and therefore the training)!\n",
    "    # Computing an observation can take 10-20 times longer as without it!\n",
    "    \"aido_wrapper\": False,\n",
    "    # RLlib only allows unknown keys in the env config -> important \"global\" keys are kept/copied here\n",
    "    \"wandb\":{\n",
    "      \"project\": 'duckietown-rllib'},\n",
    "    \"experiment_name\": 'experiment',\n",
    "    \"seed\": \"0000\",\n",
    "    \"domain_adaptation\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Ray config\n",
    "\n",
    "Here we allocate the resurser for the Ray trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_config = {\n",
    "    \"timesteps_total\": 1.e+4,\n",
    "    \n",
    "    \"ray_init_config\" :{\n",
    "#         \"address\": 127.0.0.1,\n",
    "        \"num_cpus\": 17,\n",
    "#         \"webui_host\": 127.0.0.1\n",
    "    }\n",
    ",\n",
    "    # To load a trained model to continue training DO NOT USE THIS OPTION FOR PRETRAINING\n",
    "    # -1 means no trained models are restored, training starts from random weights\n",
    "    \"restore_seed\": -1,\n",
    "    # If multiple trainings/experiments ran with the same seed\n",
    "    \"restore_experiment_idx\": 0,\n",
    "    # Every training saves two checkpoints the best (reward) and the final\n",
    "    # If the final is the best, it's the only one\n",
    "    # Otherwise the best checkpoint is saved earlier than the final, thus pretrained_checkpoint_idx: 0 --> best, 1 --> final\n",
    "    # For trainings with rllib sweeps, grid_searches, the checkpoints from all trials are loaded from the same 'list'\n",
    "    #   pretrained_checkpoint_idx: 0 --> 1st trial best checkpoint\n",
    "    #   pretrained_checkpoint_idx: 1 --> 1st trial final checkpoint\n",
    "    #   pretrained_checkpoint_idx: 2 --> 2st trial best checkpoint\n",
    "    #   pretrained_checkpoint_idx: 4 --> 2st trial final checkpoint\n",
    "    #   etc.\n",
    "    # Pay attention to trials with only one checkpoint (last = best)\n",
    "    \"restore_checkpoint_idx\": 0,\n",
    "\n",
    "\n",
    "\n",
    "    # For debugging on systems with less ram and vram\n",
    "    \"debug_hparams\":{\n",
    "      \"rllib_config\":{\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0},\n",
    "    #    train_batch_size: 64\n",
    "    #    sgd_minibatch_size: 32\n",
    "    #    eager: true\n",
    "    #    log_level: 'DEBUG'\n",
    "    #    num_sgd_iter: 2\n",
    "      \"ray_init_config\":{\n",
    "        \"num_cpus\": 1,\n",
    "        \"memory\": 2097152000, # 2000 * 1024 * 1024\n",
    "        \"object_store_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"redis_max_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"local_mode\": True}},\n",
    "\n",
    "\n",
    "    \"inference_hparams\": {\n",
    "      \"rllib_config\": {\n",
    "        \"explore\": 'false',\n",
    "        \"num_workers\": 0,\n",
    "        \"num_gpus\": 0,\n",
    "        \"callbacks\": {}},\n",
    "      \"ray_init_config\":{\n",
    "        \"num_cpus\": 1,\n",
    "        \"memory\": 2097152000, # 2000 * 1024 * 1024\n",
    "        \"object_store_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"redis_max_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"local_mode\": True}}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 PPO config\n",
    "\n",
    "Here the hyperparameters for the Ray RLlib PPO algorithm is configured.\n",
    "A perfectly detailed description on their site: [https://docs.ray.io/en/master/rllib-algorithms.html#ppo]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ppo_config = {\n",
    "  # === RLlib common congfig ================================================\n",
    "  # https://ray.readthedocs.io/en/latest/rllib-training.html#common-parameters\n",
    "  # Number of rollout worker actors to create for parallel sampling. Setting\n",
    "  # this to 0 will force rollouts to be done in the trainer actor.\n",
    "  \"num_workers\": 8,\n",
    "  # Default sample batch size (unroll length). Batches of this size are\n",
    "  # collected from rollout workers until train_batch_size is met.\n",
    "  # \"sample_batch_size\": 265,\n",
    "  # Number of GPUs to allocate to the trainer process. This can be fractional\n",
    "  # (e.g., 0.3 GPUs).\n",
    "  \"num_gpus\": 1,\n",
    "  # Training batch size, if applicable. Should be >= sample_batch_size.\n",
    "  # Samples batches will be concatenated together to a batch of this size,\n",
    "  # which is then passed to SGD.\n",
    "  \"train_batch_size\": 1024,\n",
    "  #  Environmnet name\n",
    "  'env': 'Duckietown',\n",
    "  # Env convig\n",
    "  \"env_config\": environment_config, \n",
    "  # Use calbbacks\n",
    "  'callbacks': MyCallbacks,\n",
    "  # DL Framework\n",
    "  \"framework\": 'torch',\n",
    "  # Whether to write episode stats and videos to the agent log dir. This is\n",
    "  # typically located in ~/ray_results.\n",
    "  \"monitor\": False,\n",
    "  # Evaluate with every `evaluation_interval` training iterations.\n",
    "  # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "  # Note that evaluation is currently not parallelized, and that for Ape-X\n",
    "  # metrics are already only reported for the lowest epsilon workers.\n",
    "  \"evaluation_interval\": 25, \n",
    "  # Number of episodes to run per evaluation period. If using multiple\n",
    "  # evaluation workers, we will run at least this many episodes total.\n",
    "  \"evaluation_num_episodes\": 2,\n",
    "  # Typical usage is to pass extra args to evaluation env creator\n",
    "  # and to disable exploration by computing deterministic actions.\n",
    "  # IMPORTANT NOTE: Policy gradient algorithms are able to find the optimal\n",
    "  # policy, even if this is a stochastic one. Setting \"explore=False\" here\n",
    "  # will result in the evaluation workers not using this optimal policy!\n",
    "  \"evaluation_config\": {\"monitor\": True},\n",
    "  # This argument, in conjunction with worker_index, sets the random seed of\n",
    "  # each worker, so that identically configured trials will have identical\n",
    "  # results. This makes experiments reproducible.\n",
    "  \"seed\": 1234,\n",
    "  \n",
    "\n",
    "  # === PPO-specific config =================================================\n",
    "  # https://ray.readthedocs.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo\n",
    "  # Should use a critic as a baseline (otherwise don't use value baseline;\n",
    "  # required for using GAE).\n",
    "  \"use_critic\": True,\n",
    "  # If true, use the Generalized Advantage Estimator (GAE)\n",
    "  # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.\n",
    "  \"use_gae\": True,\n",
    "  # The GAE (lambda) parameter.\n",
    "  \"lambda\": 1.0, #0.95,\n",
    "  # Initial coefficient for KL divergence.\n",
    "  \"kl_coeff\": 0.2,\n",
    "  # Size of batches collected from each worker.\n",
    "  \"rollout_fragment_length\": 200,\n",
    "  # Total SGD batch size across all devices for SGD. This defines the\n",
    "  # minibatch size within each epoch.\n",
    "  \"sgd_minibatch_size\": 128,\n",
    "  # Whether to shuffle sequences in the batch when training (recommended).\n",
    "  \"shuffle_sequences\": True,\n",
    "  # Number of SGD iterations in each outer loop (i.e., number of epochs to\n",
    "  #  execute per train batch).\n",
    "  \"num_sgd_iter\": 30,\n",
    "  # Stepsize of SGD.\n",
    "  \"lr\": 5e-5,\n",
    "  # Learning rate schedule.\n",
    "  \"lr_schedule\": None,\n",
    "  # Coefficient of the value function loss. IMPORTANT: you must tune this if\n",
    "  # you set vf_share_layers=True inside your model's config.\n",
    "  \"vf_loss_coeff\": 1.0, #0.5, #\n",
    "   # Coefficient of the entropy regularizer.\n",
    "  \"entropy_coeff\": 0.0,\n",
    "  # Decay schedule for the entropy regularizer.\n",
    "  \"entropy_coeff_schedule\": None,\n",
    "  # PPO clip parameter.\n",
    "  \"clip_param\": 0.3,\n",
    "  # Clip param for the value function. Note that this is sensitive to the\n",
    "  # scale of the rewards. If your expected V is large, increase this.\n",
    "  \"vf_clip_param\": 10.0,\n",
    "  # If specified, clip the global norm of gradients by this amount.\n",
    "  \"grad_clip\": None, #0.5\n",
    "  # Target value for KL divergence.\n",
    "  \"kl_target\": 0.01,\n",
    "  #  Whether to rollout \"complete_episodes\" or \"truncate_episodes\".\n",
    "  \"batch_mode\": \"truncate_episodes\",\n",
    "   # Discount factor of the MDP.\n",
    "  \"gamma\": 0.99,\n",
    "\n",
    "  \n",
    "  # The Domain Adaptation wrapper uses The Encoder from the UNIT networks, thich maps the\n",
    "  # RGB observations into latent variables of size 2x2x256. This is then flattent and we receive\n",
    "  # an observation of size 1024. Thus configure the PPO deep learning networks model from the basic vision model\n",
    "  # to the following Fully Connected model:\n",
    "  \"model\" :{\"fcnet_hiddens\": [1024, 512, 256, 128],\n",
    "          \"fcnet_activation\": \"relu\",\n",
    "          \"post_fcnet_hiddens\": [],\n",
    "          \"post_fcnet_activation\": \"tanh\",\n",
    "\n",
    "          # Share layers for value function. If you set this to True, it's\n",
    "          # important to tune vf_loss_coeff.\n",
    "          \"vf_share_layers\": False, }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize Ray for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = ray_config[\"ray_init_config\"][\"num_cpus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Register Duckietown env into ray tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('Duckietown', launch_and_wrap_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(PPOTrainer,\n",
    "         stop={'timesteps_total': ray_config[\"timesteps_total\"]},\n",
    "         config=ppo_config,\n",
    "         local_dir=\"./artifacts\",\n",
    "         checkpoint_at_end=True,\n",
    "         trial_name_creator=lambda trial: trial.trainable_name,  # for PPO this will make experiment dirs start with PPO_\n",
    "         name=\"ppo_DA\",\n",
    "         keep_checkpoints_num=1,\n",
    "         checkpoint_score_attr=\"episode_reward_mean\",\n",
    "         checkpoint_freq=1,\n",
    "         loggers=[CSVLogger, TBXLogger, TensorboardImageLogger],\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this when training has finished with the new path\n",
    "checkpoint_path = \"artifacts/ppo_DA/PPO_0_2021-07-12_11-08-383cftwzpe/checkpoint_24/checkpoint-24\"\n",
    "ppo_config[\"env_config\"][\"mode\"] = 'inference'\n",
    "ppo_config[\"env_config\"][\"training_map\"] = 'map1'\n",
    "ppo_config[\"env_config\"][\"domain_rand\"] = False\n",
    "# Restore agent\n",
    "trainer = PPOTrainer(config=ppo_config)\n",
    "trainer.restore(checkpoint_path)\n",
    "evaluator = DuckietownWorldEvaluator(config['env_config'], eval_lenght_sec=15, eval_map=test_map)\n",
    "results_path = \"/selfdriving_with_sim2real/\"\n",
    "evaluator.evaluate(trainer, results_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
