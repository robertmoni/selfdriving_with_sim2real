{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Envirnoment config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = {\n",
    "    # Run mode allows to load different settings in different run modes. Available options: 'train', 'inference', 'debug'\n",
    "    \"mode\": 'debug',\n",
    "    # Length of an episode (if not terminated due to a failure)\n",
    "    \"episode_max_steps\": 500,\n",
    "    # The input image will be scaled to (height, width)\n",
    "    \"resized_input_shape\" : '(84, 84)',\n",
    "    # Crop the top part of the image\n",
    "    \"crop_image_top\": 'true',  # The top 1/top_crop_divider part of the image will be cropped off. (E.g. 3 crops the top third of the image)\n",
    "    \"top_crop_divider\": 3,\n",
    "    # Convert the image to grayscale\n",
    "    \"grayscale_image\": 'false',\n",
    "    # Stack multiple frames as input\n",
    "    \"frame_stacking\": 'true',\n",
    "    # Number of frames to stack if frame stacking is enabled\n",
    "    \"frame_stacking_depth\": 3,\n",
    "    # Apply motion blur to the images during training\n",
    "    \"motion_blur\": 'false',\n",
    "    # Map the action space to a certain type. Available options are:\n",
    "    # 'leftright', leftright_clipped, 'leftright_braking', steering_braking, 'discrete'\n",
    "    # 'heading', 'heading_smooth', 'heading_trapz', 'heading_sine', 'heading_limited',\n",
    "    \"action_type\": 'heading',\n",
    "    # Overwrite the default reward function of Gym Duckietown ('default' leaves the default reward of the gym)\n",
    "    # Available options: 'default', 'default_clipped', 'posangle', 'lane_distance'\n",
    "    \"reward_function\": 'posangle',\n",
    "    # Set Gym Duckietown's distortion parameter to generate fisheye distorted images\n",
    "    \"distortion\": 'true',\n",
    "    # How large ange deviation should be accepted when the robot is placed into the simulator\n",
    "    \"accepted_start_angle_deg\": 4,\n",
    "    \"simulation_framerate\": 30,\n",
    "    # Skip frames in the agent-environment loop and only step the environment using the last action\n",
    "    \"frame_skip\": 1,\n",
    "    # Computed actions come into effect this much later in the time period of a step.\n",
    "    # Allowed values: floats in the (0., 1.) interval or 'random' to get random values in each instance of the env\n",
    "    \"action_delay_ratio\": 0.0,\n",
    "    # Map(s) used during training. Individual map names could be specified or 'multimap1' to train on a custom map set\n",
    "    \"training_map\": 'multimap1',\n",
    "    # Use Gym Duckietown's domain randomization\n",
    "    \"domain_rand\": 'false',\n",
    "    \"dynamics_rand\": 'false',\n",
    "    \"camera_rand\": 'false',\n",
    "    # If >0.0 a new observation/frame will be the same as the previous one, with a probability of frame_repeating\n",
    "    \"frame_repeating\" : 0.0,\n",
    "    # Spawn obstacles (duckies, duckiebots, etc.) to random drivable positions on a map (with or without fixed obstacles)\n",
    "    # To spawn other duckiebots this option is not recommended, use spawn_forward_obstacle instead\n",
    "    # Type and amount of spawned obstacles is determined by the dictionary under the 'obstacles' key\n",
    "    \"spawn_obstacles\": 'false',\n",
    "    \"obstacles\" :{\n",
    "      # Keys at this level specify the type of object to be spawned.\n",
    "      # Supported options: 'duckie', 'duckiebot', 'cone', 'barrier'\n",
    "      \"duckie\" :{\n",
    "          # Density: Amount of ducks per drivable tile. Can't be larger than one currently\n",
    "          \"density\": 0.5,\n",
    "          # Non-static objects move according to their default behaviour implemented in the gym Duckietown\n",
    "          # Duckies \"walk\" back and forth (like crossing the road), duckiebots perform lane following.\n",
    "          \"static\": 'true'},\n",
    "      \"duckiebot\" :{\n",
    "          \"density\": 0,\n",
    "          \"static\": 'false'}},\n",
    "    # Spawn a duckiebot in front of the controlled robot in every episode\n",
    "    # Parameters of the robot are randomised, but settings are hardcoded in ForwardObstacleSpawnnigWrapper)\n",
    "    \"spawn_forward_obstacle\": 'false',\n",
    "    # Evaluators of the AI driving olympics use small steps to generate many frames which are overlayed to get blured images\n",
    "    # The aido wrapper implements this blur, and also implements the same dynamics simulation\n",
    "    #Warning: Using AIDOWrapper slows down the environment simulation (and therefore the training)!\n",
    "    # Computing an observation can take 10-20 times longer as without it!\n",
    "    \"aido_wrapper\": 'false',\n",
    "    # RLlib only allows unknown keys in the env config -> important \"global\" keys are kept/copied here\n",
    "    \"wandb\":{\n",
    "      \"project\": 'duckietown-rllib'},\n",
    "    \"experiment_name\": 'experiment',\n",
    "    \"seed\": seed,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. RLlib config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_config = {\n",
    "    \"timesteps_total\": 1.e+6,\n",
    "    \n",
    "    \"ray_init_config\" :{\n",
    "        \"address\": 127.0.0.1,\n",
    "        \"num_cpus\": 17,\n",
    "#         \"webui_host\": 127.0.0.1\n",
    "    }\n",
    ",\n",
    "    # To load a trained model to continue training DO NOT USE THIS OPTION FOR PRETRAINING\n",
    "    # -1 means no trained models are restored, training starts from random weights\n",
    "    \"restore_seed\": -1,\n",
    "    # If multiple trainings/experiments ran with the same seed\n",
    "    \"restore_experiment_idx\": 0,\n",
    "    # Every training saves two checkpoints the best (reward) and the final\n",
    "    # If the final is the best, it's the only one\n",
    "    # Otherwise the best checkpoint is saved earlier than the final, thus pretrained_checkpoint_idx: 0 --> best, 1 --> final\n",
    "    # For trainings with rllib sweeps, grid_searches, the checkpoints from all trials are loaded from the same 'list'\n",
    "    #   pretrained_checkpoint_idx: 0 --> 1st trial best checkpoint\n",
    "    #   pretrained_checkpoint_idx: 1 --> 1st trial final checkpoint\n",
    "    #   pretrained_checkpoint_idx: 2 --> 2st trial best checkpoint\n",
    "    #   pretrained_checkpoint_idx: 4 --> 2st trial final checkpoint\n",
    "    #   etc.\n",
    "    # Pay attention to trials with only one checkpoint (last = best)\n",
    "    \"restore_checkpoint_idx\": 0,\n",
    "\n",
    "\n",
    "\n",
    "    # For debugging on systems with less ram and vram\n",
    "    \"debug_hparams\":{\n",
    "      \"rllib_config\":{\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0},\n",
    "    #    train_batch_size: 64\n",
    "    #    sgd_minibatch_size: 32\n",
    "    #    eager: true\n",
    "    #    log_level: 'DEBUG'\n",
    "    #    num_sgd_iter: 2\n",
    "      \"ray_init_config\":{\n",
    "        \"num_cpus\": 1,\n",
    "        \"memory\": 2097152000, # 2000 * 1024 * 1024\n",
    "        \"object_store_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"redis_max_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"local_mode\": True}},\n",
    "\n",
    "\n",
    "    \"inference_hparams\": {\n",
    "      \"rllib_config\": {\n",
    "        \"explore\": 'false',\n",
    "        \"num_workers\": 0,\n",
    "        \"num_gpus\": 0,\n",
    "        \"callbacks\": {}},\n",
    "      \"ray_init_config\":{\n",
    "        \"num_cpus\": 1,\n",
    "        \"memory\": 2097152000, # 2000 * 1024 * 1024\n",
    "        \"object_store_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"redis_max_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"local_mode\": True}}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 PPO config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = {\n",
    "  # === RLlib common congfig ================================================\n",
    "  # https://ray.readthedocs.io/en/latest/rllib-training.html#common-parameters\n",
    "  # Number of rollout worker actors to create for parallel sampling. Setting\n",
    "  # this to 0 will force rollouts to be done in the trainer actor.\n",
    "  \"num_workers\": 16,\n",
    "  # Default sample batch size (unroll length). Batches of this size are\n",
    "  # collected from rollout workers until train_batch_size is met.\n",
    "  \"sample_batch_size\": 265,\n",
    "  # Number of GPUs to allocate to the trainer process. This can be fractional\n",
    "  # (e.g., 0.3 GPUs).\n",
    "  \"num_gpus\": 1,\n",
    "  # Training batch size, if applicable. Should be >= sample_batch_size.\n",
    "  # Samples batches will be concatenated together to a batch of this size,\n",
    "  # which is then passed to SGD.\n",
    "  \"train_batch_size\": 4096,\n",
    "  # Discount factor of the MDP.\n",
    "  \"gamma\": 0.99,\n",
    "  # The default learning rate.\n",
    "  # Note, that scientific notation is only interpreted as a number if the . and the sign are included!\n",
    "  \"lr\": 5.e-5,\n",
    "  # Whether to write episode stats and videos to the agent log dir. This is\n",
    "  # typically located in ~/ray_results.\n",
    "  \"monitor\": 'false',\n",
    "  # Evaluate with every `evaluation_interval` training iterations.\n",
    "  # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "  # Note that evaluation is currently not parallelized, and that for Ape-X\n",
    "  # metrics are already only reported for the lowest epsilon workers.\n",
    "  \"evaluation_interval\": None, # 25\n",
    "  # Number of episodes to run per evaluation period. If using multiple\n",
    "  # evaluation workers, we will run at least this many episodes total.\n",
    "  \"evaluation_num_episodes\": 2,\n",
    "  # Typical usage is to pass extra args to evaluation env creator\n",
    "  # and to disable exploration by computing deterministic actions.\n",
    "  # IMPORTANT NOTE: Policy gradient algorithms are able to find the optimal\n",
    "  # policy, even if this is a stochastic one. Setting \"explore=False\" here\n",
    "  # will result in the evaluation workers not using this optimal policy!\n",
    "  \"evaluation_config\": {\"monitor\": 'true'},\n",
    "  # This argument, in conjunction with worker_index, sets the random seed of\n",
    "  # each worker, so that identically configured trials will have identical\n",
    "  # results. This makes experiments reproducible.\n",
    "  \"seed\": 1234,\n",
    "  # === PPO-specific config =================================================\n",
    "  # https://ray.readthedocs.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo\n",
    "  # The GAE(lambda) parameter.\n",
    "  \"lambda\": 0.95,\n",
    "  # Total SGD batch size across all devices for SGD. This defines the\n",
    "  # minibatch size within each epoch.\n",
    "  \"sgd_minibatch_size\": 128,\n",
    "  # Coefficient of the value function loss. IMPORTANT: you must tune this if\n",
    "  # you set vf_share_layers: True. (it's False by default).\n",
    "  \"vf_loss_coeff\": 0.5,\n",
    "   # Coefficient of the entropy regularizer.\n",
    "  \"entropy_coeff\": 0.0,\n",
    "  # PPO clip parameter.\n",
    "  \"clip_param\": 0.2,\n",
    "  # Clip param for the value function. Note that this is sensitive to the\n",
    "  # scale of the rewards. If your expected V is large, increase this.\n",
    "  \"vf_clip_param\": 0.2,\n",
    "  # If specified, clip the global norm of gradients by this amount.\n",
    "  \"grad_clip\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import CSVLogger, TBXLogger\n",
    "from env import launch_and_wrap_env\n",
    "from env import launch_and_wrap_env\n",
    "from utils import seed\n",
    "from rllib_callbacks import on_episode_start, on_episode_step, on_episode_end, on_train_result\n",
    "from rllib_loggers import TensorboardImageLogger, WeightsAndBiasesLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize Ray for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-65830f5c1213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ray_init_config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/sim2real_ad/lib/python3.6/site-packages/ray/worker.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(address, redis_address, redis_port, num_cpus, num_gpus, memory, object_store_memory, resources, driver_object_store_memory, redis_max_memory, log_to_driver, node_ip_address, object_id_seed, local_mode, redirect_worker_output, redirect_output, ignore_reinit_error, num_redis_shards, redis_max_clients, redis_password, plasma_directory, huge_pages, include_java, include_webui, webui_host, job_id, configure_logging, logging_level, logging_format, plasma_store_socket_name, raylet_socket_name, temp_dir, load_code_from_local, use_pickle, _internal_config)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mredis_address\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maddress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         redis_address, _, _ = services.validate_redis_address(\n\u001b[0;32m--> 663\u001b[0;31m             address, redis_address)\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfigure_logging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sim2real_ad/lib/python3.6/site-packages/ray/services.py\u001b[0m in \u001b[0;36mvalidate_redis_address\u001b[0;34m(address, redis_address)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mredis_address\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mredis_address\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddress_to_ip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredis_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mredis_address_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredis_address\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sim2real_ad/lib/python3.6/site-packages/ray/services.py\u001b[0m in \u001b[0;36maddress_to_ip\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0maddress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \"\"\"\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0maddress_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0mip_address\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgethostbyname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress_parts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;31m# Make sure localhost isn't resolved to the loopback ip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "ray.init(ray_config[\"ray_init_config\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim2real_ad",
   "language": "python",
   "name": "sim2real_ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
