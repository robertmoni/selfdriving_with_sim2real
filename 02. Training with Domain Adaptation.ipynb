{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARN)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import CSVLogger, TBXLogger\n",
    "from rllib_utils.env import launch_and_wrap_env\n",
    "from rllib_utils.utils import seed\n",
    "from rllib_utils.rllib_callbacks import on_episode_start, on_episode_step, on_episode_end, on_train_result\n",
    "from rllib_utils.rllib_loggers import TensorboardImageLogger"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Configs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seed = 42\n",
    "environment_config = {\n",
    "    # Run mode allows to load different settings in different run modes. Available options: 'train', 'inference', 'debug'\n",
    "    \"mode\": 'debug',\n",
    "    # Length of an episode (if not terminated due to a failure)\n",
    "    \"episode_max_steps\": 500,\n",
    "    # The input image will be scaled to (height, width)\n",
    "    \"resized_input_shape\" : '(84, 84)',\n",
    "    # Crop the top part of the image\n",
    "    \"crop_image_top\": 'true',  # The top 1/top_crop_divider part of the image will be cropped off. (E.g. 3 crops the top third of the image)\n",
    "    \"top_crop_divider\": 3,\n",
    "    # Convert the image to grayscale\n",
    "    \"grayscale_image\": 'false',\n",
    "    # Stack multiple frames as input\n",
    "    \"frame_stacking\": 'true',\n",
    "    # Number of frames to stack if frame stacking is enabled\n",
    "    \"frame_stacking_depth\": 3,\n",
    "    # Apply motion blur to the images during training\n",
    "    \"motion_blur\": 'false',\n",
    "    # Map the action space to a certain type. Available options are:\n",
    "    # 'leftright', leftright_clipped, 'leftright_braking', steering_braking, 'discrete'\n",
    "    # 'heading', 'heading_smooth', 'heading_trapz', 'heading_sine', 'heading_limited',\n",
    "    \"action_type\": 'heading',\n",
    "    # Overwrite the default reward function of Gym Duckietown ('default' leaves the default reward of the gym)\n",
    "    # Available options: 'default', 'default_clipped', 'posangle', 'lane_distance'\n",
    "    \"reward_function\": 'posangle',\n",
    "    # Set Gym Duckietown's distortion parameter to generate fisheye distorted images\n",
    "    \"distortion\": 'true',\n",
    "    # How large ange deviation should be accepted when the robot is placed into the simulator\n",
    "    \"accepted_start_angle_deg\": 4,\n",
    "    \"simulation_framerate\": 20,\n",
    "    # Skip frames in the agent-environment loop and only step the environment using the last action\n",
    "    \"frame_skip\": 1,\n",
    "    # Computed actions come into effect this much later in the time period of a step.\n",
    "    # Allowed values: floats in the (0., 1.) interval or 'random' to get random values in each instance of the env\n",
    "    \"action_delay_ratio\": 0.0,\n",
    "    # Map(s) used during training. Individual map names could be specified or 'multimap1' to train on a custom map set\n",
    "    \"training_map\": 'udem1',\n",
    "    # Use Gym Duckietown's domain randomization\n",
    "    \"domain_rand\": 'false',\n",
    "    \"dynamics_rand\": 'false',\n",
    "    \"camera_rand\": 'false',\n",
    "    # If >0.0 a new observation/frame will be the same as the previous one, with a probability of frame_repeating\n",
    "    \"frame_repeating\" : 0.0,\n",
    "    # Spawn obstacles (duckies, duckiebots, etc.) to random drivable positions on a map (with or without fixed obstacles)\n",
    "    # To spawn other duckiebots this option is not recommended, use spawn_forward_obstacle instead\n",
    "    # Type and amount of spawned obstacles is determined by the dictionary under the 'obstacles' key\n",
    "    \"spawn_obstacles\": 'false',\n",
    "    \"obstacles\" :{\n",
    "      # Keys at this level specify the type of object to be spawned.\n",
    "      # Supported options: 'duckie', 'duckiebot', 'cone', 'barrier'\n",
    "      \"duckie\" :{\n",
    "          # Density: Amount of ducks per drivable tile. Can't be larger than one currently\n",
    "          \"density\": 0.5,\n",
    "          # Non-static objects move according to their default behaviour implemented in the gym Duckietown\n",
    "          # Duckies \"walk\" back and forth (like crossing the road), duckiebots perform lane following.\n",
    "          \"static\": 'true'},\n",
    "      \"duckiebot\" :{\n",
    "          \"density\": 0,\n",
    "          \"static\": 'false'}},\n",
    "    # Spawn a duckiebot in front of the controlled robot in every episode\n",
    "    # Parameters of the robot are randomised, but settings are hardcoded in ForwardObstacleSpawnnigWrapper)\n",
    "    \"spawn_forward_obstacle\": 'false',\n",
    "    # Evaluators of the AI driving olympics use small steps to generate many frames which are overlayed to get blured images\n",
    "    # The aido wrapper implements this blur, and also implements the same dynamics simulation\n",
    "    #Warning: Using AIDOWrapper slows down the environment simulation (and therefore the training)!\n",
    "    # Computing an observation can take 10-20 times longer as without it!\n",
    "    \"aido_wrapper\": 'false',\n",
    "    # RLlib only allows unknown keys in the env config -> important \"global\" keys are kept/copied here\n",
    "    \"wandb\":{\n",
    "      \"project\": 'duckietown-rllib'},\n",
    "    \"experiment_name\": 'experiment',\n",
    "    \"seed\": \"0000\",}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ray_config = {\n",
    "    \"timesteps_total\": 1.e+4,\n",
    "    \n",
    "    \"ray_init_config\" :{\n",
    "#         \"address\": 127.0.0.1,\n",
    "        \"num_cpus\": 17,\n",
    "#         \"webui_host\": 127.0.0.1\n",
    "    }\n",
    ",\n",
    "    # To load a trained model to continue training DO NOT USE THIS OPTION FOR PRETRAINING\n",
    "    # -1 means no trained models are restored, training starts from random weights\n",
    "    \"restore_seed\": -1,\n",
    "    # If multiple trainings/experiments ran with the same seed\n",
    "    \"restore_experiment_idx\": 0,\n",
    "    # Every training saves two checkpoints the best (reward) and the final\n",
    "    # If the final is the best, it's the only one\n",
    "    # Otherwise the best checkpoint is saved earlier than the final, thus pretrained_checkpoint_idx: 0 --> best, 1 --> final\n",
    "    # For trainings with rllib sweeps, grid_searches, the checkpoints from all trials are loaded from the same 'list'\n",
    "    #   pretrained_checkpoint_idx: 0 --> 1st trial best checkpoint\n",
    "    #   pretrained_checkpoint_idx: 1 --> 1st trial final checkpoint\n",
    "    #   pretrained_checkpoint_idx: 2 --> 2st trial best checkpoint\n",
    "    #   pretrained_checkpoint_idx: 4 --> 2st trial final checkpoint\n",
    "    #   etc.\n",
    "    # Pay attention to trials with only one checkpoint (last = best)\n",
    "    \"restore_checkpoint_idx\": 0,\n",
    "\n",
    "\n",
    "\n",
    "    # For debugging on systems with less ram and vram\n",
    "    \"debug_hparams\":{\n",
    "      \"rllib_config\":{\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0},\n",
    "    #    train_batch_size: 64\n",
    "    #    sgd_minibatch_size: 32\n",
    "    #    eager: true\n",
    "    #    log_level: 'DEBUG'\n",
    "    #    num_sgd_iter: 2\n",
    "      \"ray_init_config\":{\n",
    "        \"num_cpus\": 1,\n",
    "        \"memory\": 2097152000, # 2000 * 1024 * 1024\n",
    "        \"object_store_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"redis_max_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"local_mode\": True}},\n",
    "\n",
    "\n",
    "    \"inference_hparams\": {\n",
    "      \"rllib_config\": {\n",
    "        \"explore\": 'false',\n",
    "        \"num_workers\": 0,\n",
    "        \"num_gpus\": 0,\n",
    "        \"callbacks\": {}},\n",
    "      \"ray_init_config\":{\n",
    "        \"num_cpus\": 1,\n",
    "        \"memory\": 2097152000, # 2000 * 1024 * 1024\n",
    "        \"object_store_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"redis_max_memory\": 209715200, # 200 * 1024 * 1024\n",
    "        \"local_mode\": True}}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ppo_config = {\n",
    "    \n",
    "  'env': 'Duckietown',\n",
    "  'callbacks': {'on_episode_start': on_episode_start,\n",
    "                'on_episode_step': on_episode_step,\n",
    "                'on_episode_end': on_episode_end,\n",
    "                'on_train_result': on_train_result},\n",
    "  \"env_config\": environment_config,  \n",
    "  # === RLlib common congfig ================================================\n",
    "  # https://ray.readthedocs.io/en/latest/rllib-training.html#common-parameters\n",
    "  # Number of rollout worker actors to create for parallel sampling. Setting\n",
    "  # this to 0 will force rollouts to be done in the trainer actor.\n",
    "  \"num_workers\": 8,\n",
    "  # Default sample batch size (unroll length). Batches of this size are\n",
    "  # collected from rollout workers until train_batch_size is met.\n",
    "  \"sample_batch_size\": 265,\n",
    "  # Number of GPUs to allocate to the trainer process. This can be fractional\n",
    "  # (e.g., 0.3 GPUs).\n",
    "  \"num_gpus\": 1,\n",
    "  # Training batch size, if applicable. Should be >= sample_batch_size.\n",
    "  # Samples batches will be concatenated together to a batch of this size,\n",
    "  # which is then passed to SGD.\n",
    "  \"train_batch_size\": 4096,\n",
    "  # Discount factor of the MDP.\n",
    "  \"gamma\": 0.99,\n",
    "  # The default learning rate.\n",
    "  # Note, that scientific notation is only interpreted as a number if the . and the sign are included!\n",
    "  \"lr\": 5.e-5,\n",
    "  # Whether to write episode stats and videos to the agent log dir. This is\n",
    "  # typically located in ~/ray_results.\n",
    "  \"monitor\": 'false',\n",
    "  # Evaluate with every `evaluation_interval` training iterations.\n",
    "  # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "  # Note that evaluation is currently not parallelized, and that for Ape-X\n",
    "  # metrics are already only reported for the lowest epsilon workers.\n",
    "  \"evaluation_interval\": None, # 25\n",
    "  # Number of episodes to run per evaluation period. If using multiple\n",
    "  # evaluation workers, we will run at least this many episodes total.\n",
    "  \"evaluation_num_episodes\": 2,\n",
    "  # Typical usage is to pass extra args to evaluation env creator\n",
    "  # and to disable exploration by computing deterministic actions.\n",
    "  # IMPORTANT NOTE: Policy gradient algorithms are able to find the optimal\n",
    "  # policy, even if this is a stochastic one. Setting \"explore=False\" here\n",
    "  # will result in the evaluation workers not using this optimal policy!\n",
    "  \"evaluation_config\": {\"monitor\": 'true'},\n",
    "  # This argument, in conjunction with worker_index, sets the random seed of\n",
    "  # each worker, so that identically configured trials will have identical\n",
    "  # results. This makes experiments reproducible.\n",
    "  \"seed\": 1234,\n",
    "  # === PPO-specific config =================================================\n",
    "  # https://ray.readthedocs.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo\n",
    "  # The GAE(lambda) parameter.\n",
    "  \"lambda\": 0.95,\n",
    "  # Total SGD batch size across all devices for SGD. This defines the\n",
    "  # minibatch size within each epoch.\n",
    "  \"sgd_minibatch_size\": 128,\n",
    "  # Coefficient of the value function loss. IMPORTANT: you must tune this if\n",
    "  # you set vf_share_layers: True. (it's False by default).\n",
    "  \"vf_loss_coeff\": 0.5,\n",
    "   # Coefficient of the entropy regularizer.\n",
    "  \"entropy_coeff\": 0.0,\n",
    "  # PPO clip parameter.\n",
    "  \"clip_param\": 0.2,\n",
    "  # Clip param for the value function. Note that this is sensitive to the\n",
    "  # scale of the rewards. If your expected V is large, increase this.\n",
    "  \"vf_clip_param\": 0.2,\n",
    "  # If specified, clip the global norm of gradients by this amount.\n",
    "  \"grad_clip\": 0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ray.init(num_cpus = ray_config[\"ray_init_config\"][\"num_cpus\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "register_env('Duckietown', launch_and_wrap_env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tune.run(PPOTrainer,\n",
    "         stop={'timesteps_total': ray_config[\"timesteps_total\"]},\n",
    "         config=ppo_config,\n",
    "         local_dir=\"./artifacts\",\n",
    "         checkpoint_at_end=True,\n",
    "         trial_name_creator=lambda trial: trial.trainable_name,  # for PPO this will make experiment dirs start with PPO_\n",
    "         name=\"ppo_DA\",\n",
    "         keep_checkpoints_num=1,\n",
    "         checkpoint_score_attr=\"episode_reward_mean\",\n",
    "         checkpoint_freq=1,\n",
    "         loggers=[CSVLogger, TBXLogger, TensorboardImageLogger],\n",
    "#          verbose = 1,\n",
    "         )"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim2real_ad",
   "language": "python",
   "name": "sim2real_ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}