{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Background processes not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a45a0979fa12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Xvfb :0 -screen 0 1024x768x24 -ac +extension GLX +render -noreset &> xvfb.log &'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DISPLAY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m':0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;31m# os.system() or use ip.system=ip.system_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0;31m# if they really want a background process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Background processes not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;31m# we explicitly do NOT return the subprocess status code, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Background processes not supported."
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import CSVLogger, TBXLogger\n",
    "from rllib_utils.env import launch_and_wrap_env\n",
    "from rllib_utils.utils import seed\n",
    "from rllib_utils.rllib_callbacks import MyCallbacks\n",
    "from rllib_utils.rllib_loggers import TensorboardImageLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure 3 different tools:\n",
    "- 1. Duckietown environment\n",
    "- 2. Ray Tune\n",
    "- 3. Ray PPO hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Envirnoment config\n",
    "\n",
    "The Gym-Duckietown environment provides a wide palette of possible configurations.\n",
    "Most of the chosen config settings is detailed in the below setup.\n",
    "\n",
    "For more details visit: https://github.com/duckietown/gym-duckietown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "environment_config = {\n",
    "    # Run mode allows to load different settings in different run modes. Available options: 'train', 'inference', 'debug'\n",
    "    \"mode\": 'train',\n",
    "    # Length of an episode (if not terminated due to a failure)\n",
    "    \"episode_max_steps\": 500,\n",
    "    # The input image will be scaled to (height, width)\n",
    "    \"resized_input_shape\" : '(84, 84)',\n",
    "    # Crop the top part of the image\n",
    "    \"crop_image_top\": True,  # The top 1/top_crop_divider part of the image will be cropped off. (E.g. 3 crops the top third of the image)\n",
    "    \"top_crop_divider\": 3,\n",
    "    # Convert the image to grayscale\n",
    "    \"grayscale_image\": False,\n",
    "    # Stack multiple frames as input\n",
    "    \"frame_stacking\": True,\n",
    "    # Number of frames to stack if frame stacking is enabled\n",
    "    \"frame_stacking_depth\": 3,\n",
    "    # Apply motion blur to the images during training\n",
    "    \"motion_blur\": False,\n",
    "    # Map the action space to a certain type. Available options are:\n",
    "    # 'leftright', leftright_clipped, 'leftright_braking', steering_braking, 'discrete'\n",
    "    # 'heading', 'heading_smooth', 'heading_trapz', 'heading_sine', 'heading_limited',\n",
    "    \"action_type\": 'heading',\n",
    "    # Overwrite the default reward function of Gym Duckietown ('default' leaves the default reward of the gym)\n",
    "    # Available options: 'default', 'default_clipped', 'posangle', 'lane_distance'\n",
    "    \"reward_function\": 'posangle',\n",
    "    # Set Gym Duckietown's distortion parameter to generate fisheye distorted images\n",
    "    \"distortion\": True,\n",
    "    # How large ange deviation should be accepted when the robot is placed into the simulator\n",
    "    \"accepted_start_angle_deg\": 4,\n",
    "    \"simulation_framerate\": 20,\n",
    "    # Skip frames in the agent-environment loop and only step the environment using the last action\n",
    "    \"frame_skip\": 1,\n",
    "    # Computed actions come into effect this much later in the time period of a step.\n",
    "    # Allowed values: floats in the (0., 1.) interval or 'random' to get random values in each instance of the env\n",
    "    \"action_delay_ratio\": 0.0,\n",
    "    # Map(s) used during training. Individual map names could be specified or 'multimap1' to train on a custom map set\n",
    "    \"training_map\": 'udem1',\n",
    "    # Use Gym Duckietown's domain randomization\n",
    "    \"domain_rand\": True,\n",
    "    \"dynamics_rand\": False,\n",
    "    \"camera_rand\": False,\n",
    "    # If >0.0 a new observation/frame will be the same as the previous one, with a probability of frame_repeating\n",
    "    \"frame_repeating\" : 0.0,\n",
    "    # Spawn obstacles (duckies, duckiebots, etc.) to random drivable positions on a map (with or without fixed obstacles)\n",
    "    # To spawn other duckiebots this option is not recommended, use spawn_forward_obstacle instead\n",
    "    # Type and amount of spawned obstacles is determined by the dictionary under the 'obstacles' key\n",
    "    \"spawn_obstacles\": False,\n",
    "    \"obstacles\" :{\n",
    "      # Keys at this level specify the type of object to be spawned.\n",
    "      # Supported options: 'duckie', 'duckiebot', 'cone', 'barrier'\n",
    "      \"duckie\" :{\n",
    "          # Density: Amount of ducks per drivable tile. Can't be larger than one currently\n",
    "          \"density\": 0.5,\n",
    "          # Non-static objects move according to their default behaviour implemented in the gym Duckietown\n",
    "          # Duckies \"walk\" back and forth (like crossing the road), duckiebots perform lane following.\n",
    "          \"static\": True},\n",
    "      \"duckiebot\" :{\n",
    "          \"density\": 0,\n",
    "          \"static\": False}},\n",
    "    # Spawn a duckiebot in front of the controlled robot in every episode\n",
    "    # Parameters of the robot are randomised, but settings are hardcoded in ForwardObstacleSpawnnigWrapper)\n",
    "    \"spawn_forward_obstacle\": False,\n",
    "    # Evaluators of the AI driving olympics use small steps to generate many frames which are overlayed to get blured images\n",
    "    # The aido wrapper implements this blur, and also implements the same dynamics simulation\n",
    "    #Warning: Using AIDOWrapper slows down the environment simulation (and therefore the training)!\n",
    "    # Computing an observation can take 10-20 times longer as without it!\n",
    "    \"aido_wrapper\": False,\n",
    "    # RLlib only allows unknown keys in the env config -> important \"global\" keys are kept/copied here\n",
    "    \"wandb\":{\n",
    "      \"project\": 'duckietown-rllib'},\n",
    "    \"experiment_name\": 'experiment',\n",
    "    \"seed\": \"0000\",\n",
    "    \"domain_adaptation\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Ray config\n",
    "\n",
    "Here we allocate the resurser for the Ray trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_config = {\n",
    "    # Total number of iterations (min 1.000.000 recomended)\n",
    "    \"timesteps_total\": 1.e+6,\n",
    "    \n",
    "    \"ray_init_config\" :{\n",
    "        # number of CPUs allocated. Always Should be NUM_WORKERS+1 !!!\n",
    "        \"num_cpus\": 5,\n",
    "        # Shared memeory allocation\n",
    "        'object_store_memory': 8589934592, #8Gb\n",
    "        #GPU allocation (not mandatory, you can use only CPUs)\n",
    "        \"num_gpus\": 1},\n",
    "\n",
    "    # To load a trained model to continue training DO NOT USE THIS OPTION FOR PRETRAINING\n",
    "    # -1 means no trained models are restored, training starts from random weights\n",
    "    \"restore_seed\": -1,\n",
    "    # If multiple trainings/experiments ran with the same seed\n",
    "    \"restore_experiment_idx\": 0,\n",
    "    # Every training saves two checkpoints the best (reward) and the final\n",
    "    # Pay attention to trials with only one checkpoint (last = best)\n",
    "    \"restore_checkpoint_idx\": 0,\n",
    "\n",
    "    # For debugging on systems with less ram and vram\n",
    "    \"debug_hparams\":{\n",
    "      \"rllib_config\":{\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0},\n",
    "      \"ray_init_config\":{\n",
    "        \"num_cpus\": 1,\n",
    "        \"memory\": 8589934592, #8Gb\n",
    "        \"object_store_memory\": 209715200, # 200Mb\n",
    "        \"redis_max_memory\": 209715200, # 200Mb\n",
    "        \"local_mode\": True}},\n",
    "\n",
    "\n",
    "    \"inference_hparams\": {\n",
    "      \"rllib_config\": {\n",
    "        \"explore\": 'false',\n",
    "        \"num_workers\": 0,\n",
    "        \"num_gpus\": 0,\n",
    "        \"callbacks\": {}},\n",
    "      \"ray_init_config\":{\n",
    "        \"num_cpus\": 1,\n",
    "        \"memory\": 8589934592, #8Gb\n",
    "        \"object_store_memory\": 209715200, # 200Mb\n",
    "        \"redis_max_memory\": 209715200, # 200Mb\n",
    "        \"local_mode\": True}}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 PPO config\n",
    "\n",
    "Here the hyperparameters for the Ray RLlib PPO algorithm is configured.\n",
    "A perfectly detailed description on their site: [https://docs.ray.io/en/master/rllib-algorithms.html#ppo]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ppo_config = {\n",
    "  \n",
    "  # === RLlib common congfig ================================================\n",
    "  # https://ray.readthedocs.io/en/latest/rllib-training.html#common-parameters\n",
    "  # Number of rollout worker actors to create for parallel sampling. Setting\n",
    "  # this to 0 will force rollouts to be done in the trainer actor.\n",
    "  \"num_workers\": 4,\n",
    "  # Default sample batch size (unroll length). Batches of this size are\n",
    "  # collected from rollout workers until train_batch_size is met.\n",
    "  # \"sample_batch_size\": 265,\n",
    "  # Number of GPUs to allocate to the trainer process. This can be fractional\n",
    "  # (e.g., 0.3 GPUs).\n",
    "  \"num_gpus\": 1,\n",
    "  # Training batch size, if applicable. Should be >= sample_batch_size.\n",
    "  # Samples batches will be concatenated together to a batch of this size,\n",
    "  # which is then passed to SGD.\n",
    "  \"train_batch_size\": 1024,\n",
    "  #  Environmnet name\n",
    "  'env': 'Duckietown',\n",
    "  # Env convig\n",
    "  \"env_config\": environment_config, \n",
    "  # Use calbbacks\n",
    "  'callbacks': MyCallbacks,\n",
    "  # DL Framework\n",
    "  \"framework\": 'torch',\n",
    "  # Whether to write episode stats and videos to the agent log dir. This is\n",
    "  # typically located in ~/ray_results.\n",
    "  \"monitor\": False,\n",
    "  # Evaluate with every `evaluation_interval` training iterations.\n",
    "  # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "  # Note that evaluation is currently not parallelized, and that for Ape-X\n",
    "  # metrics are already only reported for the lowest epsilon workers.\n",
    "  \"evaluation_interval\": 25, \n",
    "  # Number of episodes to run per evaluation period. If using multiple\n",
    "  # evaluation workers, we will run at least this many episodes total.\n",
    "  \"evaluation_num_episodes\": 2,\n",
    "  # Typical usage is to pass extra args to evaluation env creator\n",
    "  # and to disable exploration by computing deterministic actions.\n",
    "  # IMPORTANT NOTE: Policy gradient algorithms are able to find the optimal\n",
    "  # policy, even if this is a stochastic one. Setting \"explore=False\" here\n",
    "  # will result in the evaluation workers not using this optimal policy!\n",
    "  \"evaluation_config\": {\"monitor\": True},\n",
    "  # This argument, in conjunction with worker_index, sets the random seed of\n",
    "  # each worker, so that identically configured trials will have identical\n",
    "  # results. This makes experiments reproducible.\n",
    "  \"seed\": 1234,\n",
    "  'env': 'Duckietown',\n",
    "  'callbacks':  MyCallbacks,\n",
    "  \"env_config\": environment_config,  \n",
    " \n",
    "  # === PPO-specific config =================================================\n",
    "  # https://ray.readthedocs.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo\n",
    "  # Should use a critic as a baseline (otherwise don't use value baseline;\n",
    "  # required for using GAE).\n",
    "  \"use_critic\": True,\n",
    "  # If true, use the Generalized Advantage Estimator (GAE)\n",
    "  # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.\n",
    "  \"use_gae\": True,\n",
    "  # The GAE (lambda) parameter.\n",
    "  \"lambda\": 0.95,\n",
    "  # Initial coefficient for KL divergence.\n",
    "  \"kl_coeff\": 0.2,\n",
    "  # Size of batches collected from each worker.\n",
    "  \"rollout_fragment_length\": 200,\n",
    "  # Total SGD batch size across all devices for SGD. This defines the\n",
    "  # minibatch size within each epoch.\n",
    "  \"sgd_minibatch_size\": 128,\n",
    "  # Whether to shuffle sequences in the batch when training (recommended).\n",
    "  \"shuffle_sequences\": True,\n",
    "  # Number of SGD iterations in each outer loop (i.e., number of epochs to\n",
    "  #  execute per train batch).\n",
    "  \"num_sgd_iter\": 30,\n",
    "  # Stepsize of SGD.\n",
    "  \"lr\": 5e-5,\n",
    "  # Learning rate schedule.\n",
    "  \"lr_schedule\": None,\n",
    "  # Coefficient of the value function loss. IMPORTANT: you must tune this if\n",
    "  # you set vf_share_layers=True inside your model's config.\n",
    "  \"vf_loss_coeff\": 1.0, #0.5, #\n",
    "   # Coefficient of the entropy regularizer.\n",
    "  \"entropy_coeff\": 0.0,\n",
    "  # Decay schedule for the entropy regularizer.\n",
    "  \"entropy_coeff_schedule\": None,\n",
    "  # PPO clip parameter.\n",
    "  \"clip_param\": 0.3,\n",
    "  # Clip param for the value function. Note that this is sensitive to the\n",
    "  # scale of the rewards. If your expected V is large, increase this.\n",
    "  \"vf_clip_param\": 10.0,\n",
    "  # If specified, clip the global norm of gradients by this amount.\n",
    "  \"grad_clip\": 0.5,\n",
    "  # Target value for KL divergence.\n",
    "  \"kl_target\": 0.01,\n",
    "  #  Whether to rollout \"complete_episodes\" or \"truncate_episodes\".\n",
    "  \"batch_mode\": \"truncate_episodes\",\n",
    "   # Discount factor of the MDP.\n",
    "  \"gamma\": 0.99,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize Ray for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(**ray_config[\"ray_init_config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Register Duckietown env into ray tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('Duckietown', launch_and_wrap_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tune.run(PPOTrainer,\n",
    "         stop={'timesteps_total': ray_config[\"timesteps_total\"]},\n",
    "         config=ppo_config,\n",
    "         local_dir=\"./artifacts/\",\n",
    "         checkpoint_at_end=True,\n",
    "         trial_name_creator=lambda trial: trial.trainable_name,  # for PPO this will make experiment dirs start with PPO_\n",
    "         name=\"ppo_DR\",\n",
    "         keep_checkpoints_num=1,\n",
    "         checkpoint_score_attr=\"episode_reward_mean\",\n",
    "         checkpoint_freq=1,\n",
    "         loggers=[CSVLogger, TBXLogger, TensorboardImageLogger],\n",
    "         verbose = 1,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this when training has finished\n",
    "checkpoint_path = \"artifacts/ppo_DA/PPO_0_2021-07-12_11-08-383cftwzpe/checkpoint_24/checkpoint-24\"\n",
    "ppo_config[\"env_config\"][\"mode\"] = 'inference'\n",
    "ppo_config[\"env_config\"][\"training_map\"] = 'map1'\n",
    "ppo_config[\"env_config\"][\"domain_rand\"] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore agent\n",
    "trainer = PPOTrainer(config=ppo_config)\n",
    "trainer.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot trajectories and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = DuckietownWorldEvaluator(config['env_config'], eval_lenght_sec=15, eval_map=test_map)\n",
    "results_path = \"/selfdriving_with_sim2real/\"\n",
    "evaluator.evaluate(trainer, results_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
